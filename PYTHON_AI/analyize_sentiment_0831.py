import warnings
import pandas as pd
from torch import nn
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification
import os
import glob
import torch
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

# pandasì—ì„œ openpyxl ê´€ë ¨ ê²½ê³  ë¬´ì‹œ
warnings.simplefilter(action='ignore', category=UserWarning)


# ---------------------------------------------------------
# 1. í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (âœ… ìˆ˜ì •ëœ ë¶€ë¶„)
# ---------------------------------------------------------
class TextCommentDataset(Dataset):
    def __init__(self, data_folder_path, nli_tokenizer, nli_model, device):
        self.device = device

        file_paths = glob.glob(os.path.join(data_folder_path, '**', '*.*'), recursive=True)
        file_paths = [f for f in file_paths if os.path.isfile(f) and not os.path.basename(f).startswith('.')]

        all_dfs = []
        for p in file_paths:
            try:
                if p.endswith('.csv'):
                    try:
                        all_dfs.append(pd.read_csv(p, header=None, skiprows=1, encoding='utf-8-sig'))
                    except UnicodeDecodeError:
                        try:
                            all_dfs.append(pd.read_csv(p, header=None, skiprows=1, encoding='cp949'))
                        except UnicodeDecodeError:
                            all_dfs.append(pd.read_csv(p, header=None, skiprows=1, encoding='utf-8', errors='replace'))
                elif p.endswith('.xlsx'):
                    all_dfs.append(pd.read_excel(p, header=None, skiprows=1))
            except Exception as e:
                print(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {p}, ì˜¤ë¥˜: {e}")

        if not all_dfs:
            combined_df = pd.DataFrame()
        else:
            combined_df = pd.concat(all_dfs, ignore_index=True)

        if combined_df.empty:
            self.comments = []
            self.labels = []
            return

        self.comments = combined_df[0].fillna('').tolist()

        # ğŸ”¹ ì œë¡œìƒ· NLI ëª¨ë¸ë¡œ ë¼ë²¨ ìƒì„±
        print("\nğŸ”¹ ì œë¡œìƒ· NLI ëª¨ë¸ë¡œ ëª¨ë“  ëŒ“ê¸€ì˜ ë¼ë²¨ì„ ìƒì„±í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        hypotheses = {
            "ì¹­ì°¬": "ì´ ë¬¸ì¥ì€ ì‘í’ˆì— ëŒ€í•œ ì¹­ì°¬ì´ë‹¤.",
            "ë¹„íŒ": "ì´ ë¬¸ì¥ì€ ì‘í’ˆì— ëŒ€í•œ ë¹„íŒì´ë‹¤.",
            "ì‘í’ˆë‚´ìš©": "ì´ ë¬¸ì¥ì€ ì‘í’ˆê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ë‹¤.",
            "ë…¼ìŸ": "ì´ ë¬¸ì¥ì€ ëŒ“ê¸€ì—¬ë¡ ê³¼ ë…¼ìŸì„ í¼ì¹˜ê³  ìˆë‹¤.",
            "ê´€ë ¨ì—†ìŒ": "ì´ ë¬¸ì¥ì€ ì‘í’ˆì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤",
        }
        try:
            entailment_index = nli_model.config.label2id['entailment']
        except KeyError:
            entailment_index = 0

        predicted_labels = []
        total_comments = len(self.comments)
        for i, comment in enumerate(self.comments):
            if (i + 1) % 10 == 0:
                print(f"  - ë¼ë²¨ë§ ì§„í–‰ ì¤‘: {i + 1} / {total_comments}")

            if not comment.strip():
                predicted_labels.append(-1)  # ìœ íš¨í•˜ì§€ ì•Šì€ ë¼ë²¨
                continue

            entailment_scores = {}
            for label_name, hypothesis in hypotheses.items():
                inputs = nli_tokenizer(comment, hypothesis, truncation=True, return_tensors="pt").to(device)
                with torch.no_grad():
                    logits = nli_model(**inputs).logits
                    entailment_scores[label_name] = logits[0][entailment_index].item()

            best_label = max(entailment_scores, key=entailment_scores.get)
            label_map = {"ì¹­ì°¬": 0, "ë¹„íŒ": 1, "ì‘í’ˆë‚´ìš©": 2, "ë…¼ìŸ": 3, "ê´€ë ¨ì—†ìŒ": 4}
            predicted_labels.append(label_map[best_label])

        self.labels = predicted_labels
        print("âœ… ëª¨ë“  ëŒ“ê¸€ì˜ ë¼ë²¨ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, idx):
        # âœ… í•µì‹¬ ìˆ˜ì •: í•™ìŠµì— í•„ìš”í•œ 'ëŒ“ê¸€ ì›ë³¸'ê³¼ 'ë¼ë²¨'ë§Œ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
        return {
            'comment': self.comments[idx],
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }


# ---------------------------------------------------------
# 2. í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ í´ë˜ìŠ¤
# ---------------------------------------------------------
class TextClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes=5):
        super().__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)
        return logits


# ---------------------------------------------------------
# 3. K-Fold í•™ìŠµ ì§„í–‰ (âœ… ìˆ˜ì •ëœ ë¶€ë¶„)
# ---------------------------------------------------------
def train_with_kfold(data_folder_path, epochs=5, k=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n--- í˜„ì¬ ì‚¬ìš© ì¥ì¹˜: {device} ---")

    # í•™ìŠµì— ì‚¬ìš©í•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
    MODEL_NAME = "klue/bert-base"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # ì œë¡œìƒ· ë¼ë²¨ë§ìš© ëª¨ë¸
    print("\n--- ì œë¡œìƒ· ë¼ë²¨ë§ ëª¨ë¸ ë¡œë”© ---")
    nli_model_name = "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
    nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)
    nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device)
    print("  - ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

    # ë°ì´í„°ì…‹ ë¡œë”© ë° ë¼ë²¨ ìƒì„±
    print("\n--- ë°ì´í„°ì…‹ ë¡œë”© ë° ë¼ë²¨ ìƒì„± ì‹œì‘ ---")
    full_dataset = TextCommentDataset(data_folder_path, nli_tokenizer, nli_model, device)
    if len(full_dataset) == 0:
        print("âš ï¸ í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    fold_accuracies = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(full_dataset)))):
        print(f"\n===== Fold {fold + 1} / {k} =====")

        train_subset_raw = Subset(full_dataset, train_idx)
        val_subset_raw = Subset(full_dataset, val_idx)

        # -1 ë¼ë²¨ì„ ê°€ì§„ ë°ì´í„°ë¥¼ í•„í„°ë§
        train_subset = [s for s in train_subset_raw if s['labels'] != -1]
        val_subset = [s for s in val_subset_raw if s['labels'] != -1]

        if not train_subset or not val_subset:
            print("  - Foldì— í•™ìŠµ ë˜ëŠ” ê²€ì¦ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        train_dl = DataLoader(train_subset, batch_size=16, shuffle=True)
        val_dl = DataLoader(val_subset, batch_size=16)

        model = TextClassifier(MODEL_NAME, num_classes=5).to(device)
        criterion = nn.CrossEntropyLoss()
        optim = torch.optim.Adam(model.parameters(), lr=5e-5)

        for ep in range(epochs):
            model.train()
            total_loss = 0.0
            for batch in train_dl:
                # âœ… í•µì‹¬ ìˆ˜ì •: ì´ì œ batch['comment']ì— ì ‘ê·¼ ê°€ëŠ¥
                # í•™ìŠµìš© í† í¬ë‚˜ì´ì €(klue/bert-base)ë¡œ ë°°ì¹˜ë§ˆë‹¤ í† í°í™”
                batch_inputs = tokenizer(batch['comment'], return_tensors='pt', padding=True, truncation=True,
                                         max_length=128).to(device)

                logits = model(batch_inputs['input_ids'], batch_inputs['attention_mask'])
                loss = criterion(logits, batch['labels'].to(device))
                optim.zero_grad()
                loss.backward()
                optim.step()
                total_loss += loss.item()

            # ê²€ì¦
            model.eval()
            preds, gts = [], []
            with torch.no_grad():
                for batch in val_dl:
                    # âœ… í•µì‹¬ ìˆ˜ì •: ê²€ì¦ ë°ì´í„°ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                    batch_inputs = tokenizer(batch['comment'], return_tensors='pt', padding=True, truncation=True,
                                             max_length=128).to(device)
                    logits = model(batch_inputs['input_ids'], batch_inputs['attention_mask'])
                    pred = torch.argmax(logits, dim=-1).cpu().tolist()
                    label = batch['labels'].tolist()
                    preds.extend(pred)
                    gts.extend(label)
            val_acc = accuracy_score(gts, preds)
            print(
                f"Fold {fold + 1} | Epoch {ep + 1}/{epochs} | Train Loss={total_loss / len(train_dl):.4f} | Val Acc={val_acc:.4f}")

        fold_accuracies.append(val_acc)

    if fold_accuracies:
        print(f"\nğŸ“Š {k}-Fold êµì°¨ê²€ì¦ í‰ê·  ì •í™•ë„: {sum(fold_accuracies) / len(fold_accuracies):.4f}")
    else:
        print("\ní•™ìŠµì„ ì™„ë£Œí•œ Foldê°€ ì—†ìŠµë‹ˆë‹¤.")


# =========================================================
# --- ë©”ì¸ ì‹¤í–‰ ---
# =========================================================
if __name__ == "__main__":
    data_folder_for_training = "data"
    train_with_kfold(data_folder_for_training, epochs=10, k=5)

