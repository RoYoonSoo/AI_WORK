import warnings
import pandas as pd
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel
import os
import glob
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# pandasì—ì„œ openpyxl ê´€ë ¨ ê²½ê³ ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ ë¬´ì‹œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
warnings.simplefilter(action='ignore', category=UserWarning)


# ---------------------------------------------------------
# 1. í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (âœ… ìˆ˜ì •ëœ ë¶€ë¶„)
# ---------------------------------------------------------
class TextCommentDataset(Dataset):
    def __init__(self, data_folder_path, tokenizer):
        self.tokenizer = tokenizer
        file_paths = glob.glob(os.path.join(data_folder_path, '**', '*.*'), recursive=True)
        file_paths = [f for f in file_paths if os.path.isfile(f) and not os.path.basename(f).startswith('.')]

        all_dfs = []
        for p in file_paths:
            try:
                # skiprows=1 ì¶”ê°€: ì²« ë²ˆì§¸ í–‰(í—¤ë”)ì„ ê±´ë„ˆë›°ê³  ì½ìŒ
                if p.endswith('.csv'):
                    all_dfs.append(pd.read_csv(p, header=None, skiprows=1))
                elif p.endswith('.xlsx'):
                    all_dfs.append(pd.read_excel(p, header=None, skiprows=1))
            except Exception as e:
                print(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {p}, ì˜¤ë¥˜: {e}")

        if not all_dfs:
            # ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ DataFrame ìƒì„±
            combined_df = pd.DataFrame()
        else:
            combined_df = pd.concat(all_dfs, ignore_index=True)

        # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
        if combined_df.empty:
            self.comments = []
            self.labels = []
            return

        self.comments = combined_df[0].fillna('').tolist()

        # í•™ìŠµí•  ë¼ë²¨ì„ Eì—´(ì¸ë±ìŠ¤ 4)ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •
        if 4 in combined_df.columns:
            self.label_map = {
                "ì¹­ì°¬": 0, "ë¹„íŒ": 1, "ì‘í’ˆë‚´ìš©": 2, "ë…¼ìŸ": 3, "ê´€ë ¨ì—†ìŒ": 4
            }
            self.labels = combined_df[4].map(self.label_map).fillna(-1).astype(int).tolist()
        else:
            # ì˜¤ë¥˜ ë©”ì‹œì§€ë„ Eì—´ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •
            raise ValueError("Eì—´ì— ë¼ë²¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 1ë‹¨ê³„(ë¼ë²¨ë§)ë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, idx):
        if self.labels[idx] == -1:
            next_idx = (idx + 1) % len(self)
            return self.__getitem__(next_idx)

        comment = self.comments[idx]
        label = self.labels[idx]
        inputs = self.tokenizer(
            comment, return_tensors='pt', truncation=True, max_length=128, padding='max_length'
        )
        return {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }


# ---------------------------------------------------------
# 2. í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# ---------------------------------------------------------
class TextClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes=5):
        super().__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)
        return logits


# ---------------------------------------------------------
# 3. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# ---------------------------------------------------------
def train_text_classifier(data_folder_path, epochs=5):
    MODEL_NAME = "klue/bert-base"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = TextClassifier(MODEL_NAME, num_classes=5)
    try:
        ds = TextCommentDataset(data_folder_path, tokenizer)
    except ValueError as e:
        print(f"âš ï¸ ë°ì´í„°ì…‹ ìƒì„± ì˜¤ë¥˜: {e}")
        return None
    if len(ds) == 0:
        print("âš ï¸ í•™ìŠµí•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `data` í´ë”ì˜ íŒŒì¼ì— í—¤ë” ì™¸ ë°ì´í„°ê°€ ìˆëŠ”ì§€, 1ë‹¨ê³„ ë¼ë²¨ë§ì´ ì˜¬ë°”ë¥´ê²Œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    dl = DataLoader(ds, batch_size=16, shuffle=True)
    criterion = nn.CrossEntropyLoss()
    optim = torch.optim.Adam(model.parameters(), lr=5e-5)
    print(f"\nğŸ”¥ ì´ {len(ds)}ê°œì˜ ëŒ“ê¸€ ë°ì´í„°ë¡œ í…ìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for ep in range(epochs):
        model.train()
        total_loss = 0.0
        processed_samples = 0
        for batch in dl:
            logits = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])
            loss = criterion(logits, batch['labels'])
            optim.zero_grad()
            loss.backward()
            optim.step()
            total_loss += loss.item() * batch['input_ids'].size(0)
            processed_samples += batch['input_ids'].size(0)

        if processed_samples > 0:
            print(f"Epoch {ep + 1}/{epochs}  loss={total_loss / processed_samples:.4f}")
        else:
            print(f"Epoch {ep + 1}/{epochs}  - í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("âœ… í…ìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    return model


# =========================================================
# --- 4. ë°ì´í„° ë¼ë²¨ë§ í•¨ìˆ˜ ì •ì˜ (âœ… ìˆ˜ì •ëœ ë¶€ë¶„) ---
# =========================================================
def label_all_csv_files():
    print("ğŸ§  ì œë¡œìƒ· NLI ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    try:
        model_name = "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        model.to(device)
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! í˜„ì¬ ì‚¬ìš© ì¥ì¹˜: {device}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    data_folder = 'data'
    if not os.path.isdir(data_folder):
        print(f"âŒ '{data_folder}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    files_to_process = glob.glob(os.path.join(data_folder, '**', '*.csv'), recursive=True)
    if not files_to_process:
        print("âš ï¸ ì²˜ë¦¬í•  CSV íŒŒì¼(.csv)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"\nì´ {len(files_to_process)}ê°œì˜ CSV íŒŒì¼ì„ ì œë¡œìƒ· NLI ëª¨ë¸ë¡œ ë¶„ì„í•˜ì—¬ ë¼ë²¨ë§í•©ë‹ˆë‹¤.")

    hypotheses = {
        "ì¹­ì°¬": "ì´ ë¬¸ì¥ì€ ì‘í’ˆì— ëŒ€í•œ ì¹­ì°¬ì´ë‹¤.",
        "ë¹„íŒ": "ì´ ë¬¸ì¥ì€ ì‘í’ˆì— ëŒ€í•œ ë¹„íŒì´ë‹¤.",
        "ì‘í’ˆë‚´ìš©": "ì´ ë¬¸ì¥ì€ ì‘í’ˆê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ë‹¤.",
        "ë…¼ìŸ": "ì´ ë¬¸ì¥ì€ ëŒ“ê¸€ì—¬ë¡ ê³¼ ë…¼ìŸì„ í¼ì¹˜ê³  ìˆë‹¤.",
        "ê´€ë ¨ì—†ìŒ": "ì´ ë¬¸ì¥ì€ ì‘í’ˆì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤",
    }

    try:
        entailment_index = model.config.label2id['entailment']
    except KeyError:
        print("âš ï¸ 'entailment' ë¼ë²¨ì„ ëª¨ë¸ ì„¤ì •ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì¸ 0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        entailment_index = 0

    for filepath in files_to_process:
        try:
            print(f"\nğŸ“„ '{filepath}' íŒŒì¼ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            # header=Noneìœ¼ë¡œ ë¨¼ì € ì „ì²´ íŒŒì¼ì„ ì½ìŒ (í—¤ë” í¬í•¨)
            try:
                df = pd.read_csv(filepath, header=None, encoding='utf-8')
            except UnicodeDecodeError:
                print("  -> utf-8 ë””ì½”ë”© ì‹¤íŒ¨. cp949ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                df = pd.read_csv(filepath, header=None, encoding='cp949')

            if len(df) < 2:
                print("  -> í—¤ë”ë§Œ ìˆê±°ë‚˜ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # 2í–‰ë¶€í„°ì˜ ëŒ“ê¸€(ì¸ë±ìŠ¤ 1ë¶€í„°)ì„ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ì‚¼ìŒ
            comments_to_process = df.iloc[1:, 0].fillna('').tolist()

            print(f"  -> {len(comments_to_process)}ê°œì˜ ëŒ“ê¸€ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
            predicted_labels = []

            for comment in comments_to_process:
                if not comment.strip():
                    predicted_labels.append("")
                    continue

                entailment_scores = {}
                for label_name, hypothesis in hypotheses.items():
                    inputs = tokenizer(comment, hypothesis, truncation=True, return_tensors="pt").to(device)
                    with torch.no_grad():
                        logits = model(**inputs).logits
                        entailment_scores[label_name] = logits[0][entailment_index].item()

                best_label = max(entailment_scores, key=entailment_scores.get)
                predicted_labels.append(best_label)

            # ì˜ˆì¸¡ëœ ë¼ë²¨ì„ Eì—´(ì¸ë±ìŠ¤ 4)ì˜ 2í–‰ë¶€í„° ì±„ì›Œë„£ìŒ
            # Eì—´ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê¸¸ì´ ì¡°ì ˆ
            if 4 not in df.columns:
                df[4] = pd.NA
            df.loc[1:, 4] = predicted_labels

            print(f"  -> ë¶„ì„ ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ì˜ 'Eì—´'ì— ì €ì¥í•©ë‹ˆë‹¤...")
            df.to_csv(filepath, index=False, header=False, encoding='utf-8-sig')
            print(f"  -> âœ… ì™„ë£Œ!")

        except Exception as e:
            print(f"  -> âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# =========================================================
# ---                 ë©”ì¸ ì½”ë“œ ì‹¤í–‰ ë¶€ë¶„                 ---
# =========================================================
if __name__ == "__main__":
    # 1ë‹¨ê³„ ì‹¤í–‰ í›„, ì•„ë˜ 2ë‹¨ê³„ ì½”ë“œì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”.

    print("========================================")
    print("### 1ë‹¨ê³„: ë°ì´í„° ë¼ë²¨ë§ ì‹œì‘ ###")
    print("========================================")
    label_all_csv_files()
    print("\n\n")
    #
    # print("========================================")
    # print("### 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì‹œì‘ ###")
    # print("========================================")
    # data_folder_for_training = "data"
    # my_trained_model = train_text_classifier(data_folder_for_training)
    #
    # if my_trained_model:
    #     print("\nğŸ‰ ë‚˜ë§Œì˜ ëŒ“ê¸€ ë¶„ì„ ëª¨ë¸ í•™ìŠµì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")