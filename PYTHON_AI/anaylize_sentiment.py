import pandas as pd
import glob
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import warnings
import openpyxl

# pandasì—ì„œ openpyxl ê´€ë ¨ ê²½ê³ ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ ë¬´ì‹œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
warnings.simplefilter(action='ignore', category=UserWarning)


# ---------------------------------------------------------
# 1. í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# ---------------------------------------------------------
class TextCommentDataset(Dataset):
    def __init__(self, data_folder_path, tokenizer):
        self.tokenizer = tokenizer
        file_paths = glob.glob(os.path.join(data_folder_path, '**', '*.*'), recursive=True)
        file_paths = [f for f in file_paths if os.path.isfile(f) and not os.path.basename(f).startswith('.')]

        all_dfs = []
        for p in file_paths:
            try:
                if p.endswith('.csv'):
                    all_dfs.append(pd.read_csv(p, header=None))
                elif p.endswith('.xlsx'):
                    all_dfs.append(pd.read_excel(p, header=None))
            except Exception as e:
                print(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {p}, ì˜¤ë¥˜: {e}")

        combined_df = pd.concat(all_dfs, ignore_index=True)
        self.comments = combined_df[0].fillna('').tolist()
        if 3 in combined_df.columns:
            # ë¼ë²¨ì´ í…ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” ìˆ«ì ë³€í™˜ì´ í•„ìš”í•¨ì„ ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” ë¼ë²¨ë§ë§Œ í•˜ë¯€ë¡œ, í•™ìŠµ ì½”ë“œ ë¶€ë¶„ì€ ì¼ë‹¨ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
            try:
                self.labels = (pd.to_numeric(combined_df[3], errors='coerce').fillna(0).values + 5).astype(int)
            except:
                raise ValueError("Dì—´ì— ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 1ë‹¨ê³„(ë¼ë²¨ë§)ë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        else:
            raise ValueError("Dì—´ì— ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 1ë‹¨ê³„(ë¼ë²¨ë§)ë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, idx):
        comment = self.comments[idx]
        label = self.labels[idx]
        inputs = self.tokenizer(
            comment, return_tensors='pt', truncation=True, max_length=128, padding='max_length'
        )
        return {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }


# ---------------------------------------------------------
# 2. í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# ---------------------------------------------------------
class TextClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes=11):
        super().__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)
        return logits


# ---------------------------------------------------------
# 3. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
# ---------------------------------------------------------
def train_text_classifier(data_folder_path, epochs=5):
    MODEL_NAME = "klue/bert-base"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = TextClassifier(MODEL_NAME, num_classes=11)
    try:
        ds = TextCommentDataset(data_folder_path, tokenizer)
    except ValueError as e:
        print(f"âš ï¸ ë°ì´í„°ì…‹ ìƒì„± ì˜¤ë¥˜: {e}")
        return None
    if len(ds) == 0:
        print("âš ï¸ í•™ìŠµí•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `data` í´ë”ì— ë¼ë²¨ë§ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    dl = DataLoader(ds, batch_size=16, shuffle=True)
    criterion = nn.CrossEntropyLoss()
    optim = torch.optim.Adam(model.parameters(), lr=5e-5)
    print(f"\nğŸ”¥ ì´ {len(ds)}ê°œì˜ ëŒ“ê¸€ ë°ì´í„°ë¡œ í…ìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for ep in range(epochs):
        model.train()
        total_loss = 0.0
        for batch in dl:
            logits = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])
            loss = criterion(logits, batch['labels'])
            optim.zero_grad()
            loss.backward()
            optim.step()
            total_loss += loss.item() * batch['input_ids'].size(0)
        print(f"Epoch {ep + 1}/{epochs}  loss={total_loss / len(ds):.4f}")
    print("âœ… í…ìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    return model


# =========================================================
# --- 4. ë°ì´í„° ë¼ë²¨ë§ í•¨ìˆ˜ ì •ì˜ (âš ï¸ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„) ---
# =========================================================
def label_all_csv_files():
    print("ğŸ§  NLI ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    try:
        device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        model_name = "alsgyu/sentiment-analysis-fine-tuned-model"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        model.to(device)
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! í˜„ì¬ ì‚¬ìš© ì¥ì¹˜: {device}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    data_folder = 'data'
    if not os.path.isdir(data_folder):
        print(f"âŒ '{data_folder}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    files_to_process = glob.glob(os.path.join(data_folder, '**', '*.csv'), recursive=True)
    if not files_to_process:
        print("âš ï¸ ì²˜ë¦¬í•  CSV íŒŒì¼(.csv)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"\nì´ {len(files_to_process)}ê°œì˜ CSV íŒŒì¼ì„ NLI ëª¨ë¸ë¡œ ë¶„ì„í•˜ì—¬ ë¼ë²¨ë§í•©ë‹ˆë‹¤.")
    for filepath in files_to_process:
        try:
            print(f"\nğŸ“„ '{filepath}' íŒŒì¼ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            try:
                df = pd.read_csv(filepath, header=None, encoding='utf-8')
            except UnicodeDecodeError:
                print("  -> utf-8 ë””ì½”ë”© ì‹¤íŒ¨. cp949ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                df = pd.read_csv(filepath, header=None, encoding='cp949')

            comments = df[0].fillna('').tolist()
            if not comments:
                print("  -> ëŒ“ê¸€ ë‚´ìš©ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            print(f"  -> {len(comments)}ê°œì˜ ëŒ“ê¸€ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
            final_labels = []

            hypothesis = "ì´ ë‚´ìš©ì€ ê¸ì •ì ì´ë‹¤."
            # ì¸ë±ìŠ¤ 0: ê¸ì •, 1: ì¤‘ë¦½, 2: ë¶€ì •
            index_to_label = {0: "ê¸ì •", 1: "ì¤‘ë¦½", 2: "ë¶€ì •"}

            for comment in comments:
                if not comment.strip():
                    final_labels.append("ì¤‘ë¦½")  # ë¹ˆ ëŒ“ê¸€ì€ "ì¤‘ë¦½"ìœ¼ë¡œ ì²˜ë¦¬
                    continue

                premise = comment
                input_data = tokenizer(premise, hypothesis, truncation=True, return_tensors="pt").to(device)

                with torch.no_grad():
                    output = model(input_data["input_ids"])

                # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ
                prediction_idx = torch.argmax(output.logits[0]).item()

                # ì¸ë±ìŠ¤ë¥¼ í•œê¸€ ë¼ë²¨ë¡œ ë³€í™˜
                final_labels.append(prediction_idx-1.5)
            df[3] = final_labels

            print(f"  -> ë¶„ì„ ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤...")
            df.to_csv(filepath, index=False, header=False, encoding='utf-8-sig')
            print(f"  -> âœ… ì™„ë£Œ!")

        except Exception as e:
            print(f"  -> âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# =========================================================
# ---                 ë©”ì¸ ì½”ë“œ ì‹¤í–‰ ë¶€ë¶„                 ---
# =========================================================
if __name__ == "__main__":
    # --- 1ë‹¨ê³„: ì›ë³¸ ë°ì´í„°ì— ë¼ë²¨ ê¸°ë¡í•˜ê¸° ---
    print("========================================")
    print("### 1ë‹¨ê³„: ë°ì´í„° ë¼ë²¨ë§ ì‹œì‘ ###")
    print("========================================")
    label_all_csv_files()
    print("\n\n")

    # # --- 2ë‹¨ê³„: ë¼ë²¨ë§ëœ ë°ì´í„°ë¡œ ë‚˜ë§Œì˜ ëª¨ë¸ í•™ìŠµí•˜ê¸° ---
    # print("========================================")
    # print("### 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì‹œì‘ ###")
    # print("========================================")
    # data_folder_for_training = "data"
    # my_trained_model = train_text_classifier(data_folder_for_training)
    #
    # if my_trained_model:
    #     print("\nğŸ‰ ë‚˜ë§Œì˜ ëŒ“ê¸€ ë¶„ì„ ëª¨ë¸ í•™ìŠµì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")