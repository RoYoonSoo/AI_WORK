import pandas as pd
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import warnings
import os
import glob
# âœ… ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ scikit-learn ì„í¬íŠ¸ ì¶”ê°€
from sklearn.metrics import classification_report, confusion_matrix
# âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•´ ì¶”ê°€
from sklearn.utils.class_weight import compute_class_weight

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')

# --- 1. í•œê¸€ í°íŠ¸ ì„¤ì • ---
try:
    font_name = fm.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
    plt.rc('font', family=font_name)
except FileNotFoundError:
    try:
        plt.rc('font', family='AppleGothic')
    except:
        print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ ì œëª©ê³¼ ì¶•ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
plt.rcParams['axes.unicode_minus'] = False


# --- 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ---
class TextClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes=5):
        super().__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)
        return logits


# --- 3. ì‹¤ì œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ìˆ˜ì •ë¨) ---
def load_data_for_interactive_al(labeled_folder, unlabeled_folder):
    """
    ë¼ë²¨ë§ëœ ë°ì´í„°ì™€ ë¼ë²¨ ì—†ëŠ” ë°ì´í„°ë¥¼ ê°ê°ì˜ í´ë”ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # 3a. ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ (data/)
    print(f"'{labeled_folder}'ì—ì„œ ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    labeled_files = glob.glob(os.path.join(labeled_folder, '*.csv'))
    if not labeled_files:
        print(f"ì˜¤ë¥˜: '{labeled_folder}' í´ë”ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    labeled_dfs = []
    for f in labeled_files:
        try:
            df = pd.read_csv(f, header=None, skiprows=1, encoding='utf-8')
            labeled_dfs.append(df)
        except UnicodeDecodeError:
            df = pd.read_csv(f, header=None, skiprows=1, encoding='cp949')
            labeled_dfs.append(df)
    labeled_df_full = pd.concat(labeled_dfs, ignore_index=True)
    labeled_df_full = labeled_df_full[[0, 3]].rename(columns={0: 'text', 3: 'label'})

    # 3b. ë¼ë²¨ ì—†ëŠ” ë°ì´í„° ë¡œë“œ (activeLearning/) - ëŒ“ê¸€ ë‚´ìš©(1ì—´)ë§Œ ê°€ì ¸ì˜´
    print(f"'{unlabeled_folder}'ì—ì„œ ë¼ë²¨ ì—†ëŠ” í›„ë³´ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    unlabeled_files = glob.glob(os.path.join(unlabeled_folder, '*.csv'))
    if not unlabeled_files:
        print(f"ê²½ê³ : '{unlabeled_folder}' í´ë”ì— íŒŒì¼ì´ ì—†ì–´, ì•¡í‹°ë¸Œ ëŸ¬ë‹ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        unlabeled_pool_df = pd.DataFrame(columns=['text'])
    else:
        unlabeled_dfs = []
        for f in unlabeled_files:
            try:
                # usecols=[0]ì„ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ì—´ë§Œ ì½ì–´ì˜´
                df = pd.read_csv(f, header=None, skiprows=1, usecols=[0], encoding='utf-8')
                unlabeled_dfs.append(df)
            except UnicodeDecodeError:
                df = pd.read_csv(f, header=None, skiprows=1, usecols=[0], encoding='cp949')
                unlabeled_dfs.append(df)
        unlabeled_pool_df = pd.concat(unlabeled_dfs, ignore_index=True)
        unlabeled_pool_df.columns = ['text']

    # 3c. í…ìŠ¤íŠ¸ ë¼ë²¨ì„ ìˆ«ì IDë¡œ ë³€í™˜
    label_map = {"ì¹­ì°¬": 0, "ë¹„íŒ": 1, "ì‘í’ˆë‚´ìš©": 2, "ë…¼ìŸ": 3, "ê´€ë ¨ì—†ìŒ": 4}
    labeled_df_full['label_id'] = labeled_df_full['label'].map(label_map)
    labeled_df_full.dropna(subset=['text', 'label_id'], inplace=True)
    labeled_df_full['label_id'] = labeled_df_full['label_id'].astype(int)

    return labeled_df_full, unlabeled_pool_df, label_map


# --- 4. ë©”ì¸ ì¸í„°ë™í‹°ë¸Œ í•¨ìˆ˜ ---
def run_interactive_active_learning():
    print("=" * 60)
    print("### âœï¸ ì¸í„°ë™í‹°ë¸Œ ì•¡í‹°ë¸Œ ëŸ¬ë‹ ë„êµ¬ ì‹œì‘ ###")
    print("=" * 60)

    # --- íŒŒë¼ë¯¸í„° ì„¤ì • ---
    LABELED_DATA_FOLDER = 'data'
    UNLABELED_DATA_FOLDER = 'activeLearning'
    TEST_SET_RATIO = 0.2
    VALIDATION_SET_RATIO = 0.2
    QUERY_SIZE = 7
    MODEL_NAME = "klue/bert-base"

    # âœ… --- GPU ì‚¬ìš© ì„¤ì • ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n--- í˜„ì¬ ì‚¬ìš© ì¥ì¹˜: {device} ---")

    # --- 4a. ë°ì´í„° ì¤€ë¹„ ë° ë¶„í•  ---
    print("\n--- 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ë¶„í•  ---")
    labeled_df_full, unlabeled_pool_df, label_map = load_data_for_interactive_al(LABELED_DATA_FOLDER,
                                                                                 UNLABELED_DATA_FOLDER)
    if labeled_df_full is None: return

    known_labeled_texts = set(labeled_df_full['text'])

    original_unlabeled_count = len(unlabeled_pool_df)
    unlabeled_pool_df = unlabeled_pool_df[~unlabeled_pool_df['text'].isin(known_labeled_texts)].reset_index(drop=True)
    if original_unlabeled_count > len(unlabeled_pool_df):
        print(
            f"  - ì¤‘ë³µ ì œê±°: activeLearning í´ë”ì—ì„œ ì´ë¯¸ ë¼ë²¨ë§ëœ ëŒ“ê¸€ {original_unlabeled_count - len(unlabeled_pool_df)}ê°œë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")

    remaining_df, test_df = train_test_split(labeled_df_full, test_size=TEST_SET_RATIO, random_state=42,
                                             stratify=labeled_df_full['label_id'])
    labeled_df, validation_df = train_test_split(remaining_df, test_size=VALIDATION_SET_RATIO / (1 - TEST_SET_RATIO),
                                                 random_state=42, stratify=remaining_df['label_id'])

    print(f"  - í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(test_df)}ê°œ (ìµœì¢… í‰ê°€ìš©)")
    print(f"  - ê²€ì¦ ì„¸íŠ¸: {len(validation_df)}ê°œ (ì¤‘ê°„ ì ê²€ìš©)")
    print(f"  - ì´ˆê¸° í•™ìŠµ ì„¸íŠ¸: {len(labeled_df)}ê°œ")
    print(f"  - ë¼ë²¨ ì—†ëŠ” í•™ìŠµ í›„ë³´(Unlabeled Pool): {len(unlabeled_pool_df)}ê°œ")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    validation_accuracies = []

    initial_labeled_count = len(labeled_df)

    num_rounds = len(unlabeled_pool_df) // QUERY_SIZE

    # âœ… --- í•µì‹¬ ìˆ˜ì •: ëª¨ë¸ì„ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„± ---
    print("\n--- 2ë‹¨ê³„: ì´ˆê¸° ëª¨ë¸ ìƒì„± ---")
    model = TextClassifier(MODEL_NAME, num_classes=5).to(device)

    print(f"\n--- 3ë‹¨ê³„: ì•¡í‹°ë¸Œ ëŸ¬ë‹ ë£¨í”„ ì‹œì‘ (ì´ {num_rounds}ë¼ìš´ë“œ ì˜ˆìƒ) ---")
    for i in range(num_rounds + 1):
        current_labels_count = len(labeled_df)
        print(f"\nğŸ”„ ë¼ìš´ë“œ {i + 1} (í˜„ì¬ í•™ìŠµ ë¼ë²¨ ìˆ˜: {current_labels_count})")

        # --- ëª¨ë¸ í•™ìŠµ (ì¶”ê°€ í•™ìŠµ) ---
        if i == 0:
            print("  - ì´ˆê¸° ë°ì´í„°ë¡œ ì²« ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        else:
            print("  - ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¶”ê°€ í•™ìŠµ(Fine-tuning)í•©ë‹ˆë‹¤...")

        train_texts = labeled_df['text'].tolist()
        train_labels = torch.tensor(labeled_df['label_id'].tolist(), dtype=torch.long)

        # âœ… --- í•µì‹¬ ìˆ˜ì •: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ---
        # ë°ì´í„°ê°€ ì ì€ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_labels.numpy()),
            y=train_labels.numpy()
        )
        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
        print(f"  - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©: {np.round(class_weights.cpu().numpy(), 2)}")

        optim = torch.optim.Adam(model.parameters(), lr=5e-6)
        # âœ… ì†ì‹¤ í•¨ìˆ˜ì— ê°€ì¤‘ì¹˜ ì ìš©
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        model.train()

        epochs = 10 if len(labeled_df) < 500 else 5
        batch_size = 16

        for epoch in range(epochs):
            permutation = torch.randperm(len(train_texts))
            for j in range(0, len(train_texts), batch_size):
                indices = permutation[j:j + batch_size]
                batch_texts = [train_texts[k] for k in indices]
                batch_labels = train_labels[indices].to(device)

                if not batch_texts: continue

                inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(
                    device)
                logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

                loss = criterion(logits, batch_labels);
                optim.zero_grad();
                loss.backward();
                optim.step()

        # --- ì„±ëŠ¥ ì¸¡ì • ---
        model.eval()
        val_texts = validation_df['text'].tolist()
        val_labels = torch.tensor(validation_df['label_id'].tolist(), dtype=torch.long).to(device)
        all_preds = []
        with torch.no_grad():
            for j in range(0, len(val_texts), 32):
                batch_texts = val_texts[j:j + 32]
                if not batch_texts: continue
                inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(
                    device)
                logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
                preds = torch.argmax(logits, dim=1)
                all_preds.append(preds)

        all_preds_tensor = torch.cat(all_preds)
        accuracy = (all_preds_tensor == val_labels).float().mean().item()
        validation_accuracies.append(accuracy)
        print(f"  ğŸ“ˆ [ëª¨ì˜ê³ ì‚¬] ê²€ì¦ ì„¸íŠ¸ ì „ì²´ ì •í™•ë„: {accuracy:.2%}")

        # --- ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ì¶œë ¥ ---
        print("\n  --- ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ---")
        y_true = val_labels.cpu().numpy()
        y_pred = all_preds_tensor.cpu().numpy()
        id_to_label_map = {v: k for k, v in label_map.items()}
        target_names = [id_to_label_map.get(i, "unknown") for i in range(len(label_map))]
        print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))

        print("  --- í˜¼ë™ í–‰ë ¬ (í–‰: ì‹¤ì œ, ì—´: ì˜ˆì¸¡) ---")
        unique_labels = sorted(np.unique(np.concatenate((y_true, y_pred))))
        cm_target_names = [id_to_label_map.get(i, "unknown") for i in unique_labels]
        cm = confusion_matrix(y_true, y_pred, labels=unique_labels)
        print(pd.DataFrame(cm, index=cm_target_names, columns=cm_target_names))
        print("-" * 25)

        if len(unlabeled_pool_df) < QUERY_SIZE: break

        # --- ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ ---
        continue_labeling = input("\në¼ë²¨ë§ì„ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if continue_labeling != 'y':
            print("ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ë¼ë²¨ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        # --- í—·ê°ˆë¦¬ëŠ” ë°ì´í„° ì„ ë³„ ---
        print("  - ëª¨ë¸ì´ ê°€ì¥ í—·ê°ˆë¦¬ëŠ” ë°ì´í„°ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤... (ë°ì´í„°ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
        unlabeled_texts = unlabeled_pool_df['text'].dropna().tolist()
        all_uncertainties = []
        inf_batch_size = 32

        with torch.no_grad():
            for j in range(0, len(unlabeled_texts), inf_batch_size):
                batch_texts = unlabeled_texts[j:j + inf_batch_size]
                if not batch_texts: continue

                inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(
                    device)
                logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

                probabilities = F.softmax(logits, dim=1)
                max_probs, _ = torch.max(probabilities, dim=1)
                uncertainty = 1 - max_probs
                all_uncertainties.append(uncertainty.cpu())

        if not all_uncertainties:
            print("  - ë¶ˆí™•ì‹¤ì„±ì„ ê³„ì‚°í•  ë°ì´í„°ê°€ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        full_uncertainty_tensor = torch.cat(all_uncertainties)
        query_indices = torch.topk(full_uncertainty_tensor, k=min(QUERY_SIZE, len(unlabeled_texts))).indices

        questions_to_label = unlabeled_pool_df.iloc[query_indices.numpy()]

        # --- â­ ì‹¤ì œ ì‚¬ìš©ì ë¼ë²¨ë§ ê³¼ì • â­ ---
        print("\n" + "-" * 20);
        print("âœï¸ ëª¨ë¸ì´ ì„ íƒí•œ ë°ì´í„°ì— ë¼ë²¨ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.");
        print("-" * 20)
        newly_labeled_rows = []
        valid_labels_text = list(label_map.keys())
        for index, row in questions_to_label.iterrows():
            while True:
                print(f"\nëŒ“ê¸€: {row['text']}")
                user_input = input(f"ë¼ë²¨ ì…ë ¥ [{'/'.join(valid_labels_text)}]: ").strip()
                if user_input in valid_labels_text:
                    new_row = {'text': row['text'], 'label': user_input, 'label_id': label_map[user_input]}
                    newly_labeled_rows.append(new_row)
                    break
                else:
                    print(">> ì˜ëª»ëœ ë¼ë²¨ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. <<")

        # --- ë°ì´í„°ì…‹ ì—…ë°ì´íŠ¸ ---
        newly_labeled_df = pd.DataFrame(newly_labeled_rows)
        labeled_df = pd.concat([labeled_df, newly_labeled_df], ignore_index=True)
        unlabeled_pool_df = unlabeled_pool_df.drop(questions_to_label.index)
        print(f"\n  âœ… ë¼ë²¨ë§ ì™„ë£Œ! í•™ìŠµ ì„¸íŠ¸ì— {len(newly_labeled_df)}ê°œì˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")

    # --- ìµœì¢… í‰ê°€ ---
    print("\n\n--- 4ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ í‰ê°€ ---")
    test_texts = test_df['text'].tolist()
    test_labels = torch.tensor(test_df['label_id'].tolist(), dtype=torch.long).to(device)
    with torch.no_grad():
        inputs = tokenizer(test_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)
        logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
        final_accuracy = (torch.argmax(logits, dim=1) == test_labels).float().mean().item()
    print(f"  ğŸ“ [ìˆ˜ëŠ¥ ì‹œí—˜] ìµœì¢… ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ìµœì¢… ì •í™•ë„: {final_accuracy:.2%}")

    # --- ê²°ê³¼ ì‹œê°í™” ---
    print("\n--- 5ë‹¨ê³„: ê²°ê³¼ ì‹œê°í™” ---")
    labeled_counts = [initial_labeled_count + i * QUERY_SIZE for i in range(len(validation_accuracies))]
    plt.figure(figsize=(10, 6));
    plt.plot(labeled_counts, validation_accuracies, marker='o', linestyle='-')
    plt.title('ì•¡í‹°ë¸Œ ëŸ¬ë‹ ë¼ë²¨ ìˆ˜ì— ë”°ë¥¸ ê²€ì¦ ì„¸íŠ¸ ì •í™•ë„ ë³€í™”');
    plt.xlabel('í•™ìŠµì— ì‚¬ìš©ëœ ë¼ë²¨ ë°ì´í„° ê°œìˆ˜')
    plt.ylabel('ê²€ì¦ ì„¸íŠ¸ ì •í™•ë„');
    plt.grid(True);
    plt.xticks(labeled_counts, rotation=45)
    plt.ylim(0, max(1.0, max(validation_accuracies) * 1.2))
    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'));
    plt.tight_layout();
    plt.show()


# --- 5. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    run_interactive_active_learning()

